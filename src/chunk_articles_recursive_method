import os
import json
import time
import re

# ===============================
# Einstellungen
# ===============================
INPUT_DIR = "D:/BAA Code/data/clean_articles"
OUTPUT_DIR = "D:/BAA Code/data/chunks_recursive_no_overlap"
os.makedirs(OUTPUT_DIR, exist_ok=True)

MIN_WORDS = 50
MAX_WORDS = 400
OVERLAP_WORDS = 0  # 0 / 50 / 100 f√ºr Experimente


# ===============================
# Hilfsfunktionen
# ===============================

def count_words(text):
    return len(text.split())


def split_into_sentences(text):
    # einfache, robuste Satztrennung
    return re.split(r'(?<=[.!?])\s+', text)


def recursive_split(text, max_words):
    """
    Zerlegt Text rekursiv in semantisch sinnvolle Chunks
    """
    if count_words(text) <= max_words:
        return [text]

    # 1Ô∏è‚É£ Erst Absatzebene
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
    if len(paragraphs) > 1:
        chunks = []
        buffer = ""

        for para in paragraphs:
            candidate = f"{buffer}\n\n{para}" if buffer else para
            if count_words(candidate) <= max_words:
                buffer = candidate
            else:
                if buffer:
                    chunks.extend(recursive_split(buffer, max_words))
                buffer = para

        if buffer:
            chunks.extend(recursive_split(buffer, max_words))

        return chunks

    # 2Ô∏è‚É£ Satzebene
    sentences = split_into_sentences(text)
    if len(sentences) > 1:
        chunks = []
        buffer = ""

        for s in sentences:
            candidate = f"{buffer} {s}".strip()
            if count_words(candidate) <= max_words:
                buffer = candidate
            else:
                if buffer:
                    chunks.append(buffer)
                buffer = s

        if buffer:
            chunks.append(buffer)

        return chunks

    # 3Ô∏è‚É£ Fallback: Wortweise splitten
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_words):
        chunks.append(" ".join(words[i:i + max_words]))

    return chunks


def apply_overlap(chunks, overlap_words):
    if overlap_words <= 0:
        return chunks

    overlapped = []
    prev_tail = []

    for chunk in chunks:
        words = chunk.split()
        merged = prev_tail + words
        overlapped.append(" ".join(merged))

        prev_tail = words[-overlap_words:]

    return overlapped


# ===============================
# Haupt-Chunking-Funktion
# ===============================
def chunk_article_recursive(title, url, author, pub_date, content, links, filename):
    all_chunks = recursive_split(content, MAX_WORDS)
    all_chunks = apply_overlap(all_chunks, OVERLAP_WORDS)

    for idx, chunk_text in enumerate(all_chunks, start=1):
        chunk_links = [
            link for link in links
            if link.get("anchor") and link["anchor"] in chunk_text
        ]

        chunk_data = {
            "title": title,
            "url": url,
            "author": author,
            "pub_date": pub_date,
            "chunk_index": idx,
            "chunk_text": chunk_text,
            "links": chunk_links
        }

        output_file = f"{os.path.splitext(filename)[0]}_chunk{idx}.json"
        output_path = os.path.join(OUTPUT_DIR, output_file)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(chunk_data, f, ensure_ascii=False, indent=4)

        print(f"üìÑ Chunk {idx} erstellt ({count_words(chunk_text)} W√∂rter)")

        time.sleep(0.05)


# ===============================
# Batch-Verarbeitung
# ===============================
def process_all_articles():
    files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".json")]

    for filename in files:
        with open(os.path.join(INPUT_DIR, filename), "r", encoding="utf-8") as f:
            article = json.load(f)

        content = article.get("content", "")
        if not content.strip():
            print(f"‚ö†Ô∏è  √úberspringe leeren Artikel: {filename}")
            continue

        chunk_article_recursive(
            title=article.get("title", ""),
            url=article.get("url", ""),
            author=article.get("author", ""),
            pub_date=article.get("pub_date", ""),
            content=content,
            links=article.get("links", []),
            filename=filename
        )

        print(f"‚úÖ {filename} verarbeitet\n")

    print("üéØ Alle Artikel wurden rekursiv gechunked.")


# ===============================
# Einstiegspunkt
# ===============================
if __name__ == "__main__":
    process_all_articles()
